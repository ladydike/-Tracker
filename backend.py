"""
MileStone Tracker - Backend Server
Claude APIë¥¼ í™œìš©í•œ íšŒì˜ë¡ Q&A ë° í”„ë¡œì íŠ¸ íŠ¸ë˜í‚¹ ì‹œìŠ¤í…œ
(ìˆ˜ì •: ì‹¤ì‹œê°„ ìš”ì•½ ê¸°ëŠ¥ ì œê±°, ê¸°ì¡´ _Summary.md íŒŒì¼ í™œìš©)
"""

import os
import re
import json
import glob
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict

# ë²¡í„° ê²€ìƒ‰ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
from sentence_transformers import SentenceTransformer

# .env íŒŒì¼ ì§ì ‘ ë¡œë“œ
ENV_PATH = Path(r"C:\Users\lenachoi\.cursor\Practice\.env")
if ENV_PATH.exists():
    with open(ENV_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                os.environ[key.strip()] = value.strip()
    print(f"[ENV] Loaded from: {ENV_PATH}")
    print(f"[ENV] API Key exists: {bool(os.environ.get('ANTHROPIC_API_KEY'))}")
else:
    print(f"[ENV] Warning: {ENV_PATH} not found")

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
import httpx
import uvicorn

# ==================== Configuration ====================
PRACTICE_DIR = Path(r"C:\Users\lenachoi\.cursor\Practice")
CLAUDE_MODEL = "claude-sonnet-4-20250514"  # ìµœì‹  ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸
ANTHROPIC_API_URL = "https://api.anthropic.com/v1/messages"
CONTEXT_FILE = PRACTICE_DIR / "context_ì¡°ì§ì •ë³´.md"

# ì¡°ì§ ë°°ê²½ ì •ë³´ ë¡œë“œ
org_context = ""
if CONTEXT_FILE.exists():
    with open(CONTEXT_FILE, 'r', encoding='utf-8') as f:
        org_context = f.read()
    print(f"[Context] ì¡°ì§ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {CONTEXT_FILE.name}")
else:
    print(f"[Context] ì¡°ì§ ì •ë³´ íŒŒì¼ ì—†ìŒ: {CONTEXT_FILE}")

# Initialize FastAPI
app = FastAPI(title="MileStone Tracker API", version="1.1.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== Vector Search ì„¤ì • ====================
print("[Vector Search] ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ í•„ìš”)")
embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸
print("[Vector Search] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# ë²¡í„° ì €ì¥ì†Œ (ë©”ëª¨ë¦¬)
vector_store = {
    "ids": [],
    "embeddings": None,  # numpy array
}

def init_vector_db():
    """ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”"""
    global vector_store
    vector_store = {"ids": [], "embeddings": None}
    print("[Vector Search] ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")

def vectorize_meetings():
    """ëª¨ë“  íšŒì˜ë¡ì„ ë²¡í„°í™”í•˜ì—¬ ì €ì¥"""
    global vector_store
    
    if not store.meetings:
        print("[Vector Search] ë²¡í„°í™”í•  íšŒì˜ë¡ì´ ì—†ìŠµë‹ˆë‹¤")
        return
    
    print(f"[Vector Search] {len(store.meetings)}ê°œ íšŒì˜ë¡ ë²¡í„°í™” ì‹œì‘...")
    
    documents = []
    ids = []
    
    for meeting_id, meeting in store.meetings.items():
        # ìš”ì•½ë³¸ì´ ìˆìœ¼ë©´ ìš”ì•½ë³¸, ì—†ìœ¼ë©´ ì›ë³¸ ì•ë¶€ë¶„
        text = meeting.summary_content if meeting.summary_content else meeting.content[:3000]
        # ì œëª©ë„ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
        combined_text = f"ì œëª©: {meeting.title}\ní´ë”: {meeting.folder}\në‚´ìš©: {text}"
        
        documents.append(combined_text)
        ids.append(meeting_id)
    
    # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
    embeddings = embedding_model.encode(documents, show_progress_bar=True)
    
    vector_store["ids"] = ids
    vector_store["embeddings"] = embeddings
    
    print(f"[Vector Search] {len(documents)}ê°œ íšŒì˜ë¡ ë²¡í„°í™” ì™„ë£Œ!")

def semantic_search(query: str, n_results: int = 10) -> List[tuple]:
    """ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)"""
    if vector_store["embeddings"] is None or len(vector_store["ids"]) == 0:
        return []
    
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embedding_model.encode([query])[0]
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    embeddings = vector_store["embeddings"]
    # ì •ê·œí™”
    query_norm = query_embedding / np.linalg.norm(query_embedding)
    embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    similarities = np.dot(embeddings_norm, query_norm)
    
    # ìƒìœ„ Nê°œ ì¸ë±ìŠ¤
    top_indices = np.argsort(similarities)[::-1][:n_results]
    
    # ê²°ê³¼ ë§¤í•‘
    search_results = []
    for idx in top_indices:
        meeting_id = vector_store["ids"][idx]
        if meeting_id in store.meetings:
            meeting = store.meetings[meeting_id]
            score = float(similarities[idx]) * 100  # 0~100 ì ìˆ˜
            if score > 20:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                search_results.append((meeting, score, ["semantic"]))
    
    return search_results

# ==================== Data Models ====================
@dataclass
class ActionItem:
    text: str
    completed: bool

@dataclass
class MeetingSection:
    title: str
    items: List[str]

@dataclass
class Meeting:
    id: str
    title: str
    filename: str
    date: str
    folder: str
    project: Optional[str]
    content: str
    summary_content: Optional[str] = None  # _Summary.md íŒŒì¼ì˜ ë‚´ìš©
    sections: List[MeetingSection] = None
    action_items: List[ActionItem] = None
    summary: str = ""  # ë¦¬ìŠ¤íŠ¸ í‘œì‹œìš© ì§§ì€ ìš”ì•½

@dataclass
class Project:
    name: str
    stage: str
    meetings: List[str]
    last_meeting: str

# ==================== Data Store ====================
class DataStore:
    def __init__(self):
        self.meetings: Dict[str, Meeting] = {}
        self.projects: Dict[str, Project] = {}
    
    def to_dict(self):
        return {
            "meetings": [asdict(m) for m in self.meetings.values()],
            "projects": [asdict(p) for p in self.projects.values()]
        }

store = DataStore()

# ==================== Known Projects ====================
KNOWN_PROJECTS = [
    'Canary', 'PalM', 'EHG', 'Tango', 'IMPACT', 'SNS', 'bluehole',
    'ë¼ì´ì§•ìœ™ìŠ¤', 'ì–¸ë…¸ìš´', 'Valor', 'ì˜¬ë¦¬ë¸ŒíŠ¸ë¦¬', 'ë”©ì»´', 'ì†Œë…¸í‹°ì¹´',
    'ì¹´ë‚˜ë¦¬', 'íŒœ', 'íƒ±ê³ ', 'ì„íŒ©íŠ¸'
]

# ==================== Parsing Functions ====================
def parse_date_from_filename(filename: str) -> str:
    match = re.search(r'(\d{6})', filename)
    if match:
        yymmdd = match.group(1)
        year = '20' + yymmdd[:2]
        month = yymmdd[2:4]
        day = yymmdd[4:6]
        return f"{year}-{month}-{day}"
    return datetime.now().strftime("%Y-%m-%d")

def extract_project_from_filename(filename: str) -> Optional[str]:
    # Normalize
    filename = filename.replace('ì¹´ë‚˜ë¦¬', 'Canary').replace('íŒœ', 'PalM')
    
    s2_match = re.search(r'S2_(.+?)\.txt$', filename)
    if s2_match:
        return s2_match.group(1)
    
    mr_match = re.search(r'(\w+)_í‚¥ì˜¤í”„|(\w+)_MR', filename)
    if mr_match:
        return mr_match.group(1) or mr_match.group(2)
    
    for proj in KNOWN_PROJECTS:
        if proj.lower() in filename.lower():
            return proj
    return None

def detect_stage(content: str) -> str:
    lower_content = content.lower()
    if any(k in lower_content for k in ['launch', 'ì¶œì‹œ', 'ëŸ°ì¹­']): return 'launch'
    if any(k in lower_content for k in ['beta', 'ë² íƒ€']): return 'beta'
    if any(k in lower_content for k in ['alpha', 'ì•ŒíŒŒ']): return 'alpha'
    if any(k in lower_content for k in ['vertical', 'ë²„í‹°ì»¬']): return 'vertical'
    return 'kickoff'

def get_folder_from_path(filepath: Path) -> str:
    parts = filepath.parts
    # MR_Team í•˜ìœ„ í´ë” ì²˜ë¦¬ (ì˜ˆ: MR_Team/101)
    if 'MR_Team' in parts:
        idx = parts.index('MR_Team')
        if idx + 1 < len(parts):
            return f"MR_Team/{parts[idx+1]}"
        return 'MR_Team'
    
    for part in ['MR_meeting', 'S2_meeting', 'ë¹„ì •ê¸°íšŒì˜']:
        if part in parts:
            return part
    return 'ê¸°íƒ€'

# ==================== File Loading ====================
def load_meetings_from_directory():
    store.meetings.clear()
    store.projects.clear()
    
    txt_files = glob.glob(str(PRACTICE_DIR / "**/*.txt"), recursive=True)
    
    for filepath in txt_files:
        path = Path(filepath)
        if path.name.endswith('_Summary.md'): continue
        # requirements.txt ë“± íšŒì˜ë¡ì´ ì•„ë‹Œ íŒŒì¼ ì œì™¸
        if path.name in ['requirements.txt', 'README.txt']: continue
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ìš”ì•½ íŒŒì¼ í™•ì¸ (_Summary.md)
            summary_path = path.with_name(path.stem + "_Summary.md")
            summary_content = None
            if summary_path.exists():
                with open(summary_path, 'r', encoding='utf-8') as f:
                    summary_content = f.read()
            
            filename = path.name
            date = parse_date_from_filename(filename)
            folder = get_folder_from_path(path)
            project = extract_project_from_filename(filename)
            
            meeting_id = f"meeting_{hash(str(path)) % 10000000}"
            
            meeting = Meeting(
                id=meeting_id,
                title=filename.replace('.txt', ''),
                filename=filename,
                date=date,
                folder=folder,
                project=project,
                content=content,
                summary_content=summary_content,
                sections=[],
                action_items=[],
                summary=content[:100].strip() + "..."
            )
            
            store.meetings[meeting_id] = meeting
            
            if project:
                if project not in store.projects:
                    store.projects[project] = Project(name=project, stage=detect_stage(content), meetings=[], last_meeting=date)
                store.projects[project].meetings.append(meeting_id)
                if date > store.projects[project].last_meeting:
                    store.projects[project].last_meeting = date
                    
        except Exception as e:
            print(f"Error loading {path}: {e}")

    print(f"Loaded {len(store.meetings)} meetings and {len(store.projects)} projects")

# ==================== API Endpoints ====================

@app.on_event("startup")
async def startup_event():
    load_meetings_from_directory()
    # ë²¡í„° DB ì´ˆê¸°í™” ë° íšŒì˜ë¡ ë²¡í„°í™”
    init_vector_db()
    vectorize_meetings()

@app.get("/")
async def root():
    return FileResponse("milestone_tracker.html")

@app.get("/api/sync")
async def sync_meetings():
    load_meetings_from_directory()
    # ë²¡í„° DBë„ ì¬ìƒì„±
    init_vector_db()
    vectorize_meetings()
    return {"status": "success", "count": len(store.meetings)}

@app.get("/api/meetings")
async def get_meetings():
    return sorted([asdict(m) for m in store.meetings.values()], key=lambda x: x["date"], reverse=True)

@app.get("/api/meetings/{meeting_id}")
async def get_meeting(meeting_id: str):
    if meeting_id not in store.meetings:
        raise HTTPException(status_code=404, detail="Meeting not found")
    return asdict(store.meetings[meeting_id])

@app.get("/api/folders")
async def get_folders():
    folders = defaultdict(int)
    for m in store.meetings.values():
        folders[m.folder] += 1
    return [{"name": k, "count": v} for k, v in sorted(folders.items())]

@app.get("/api/projects")
async def get_projects():
    return sorted([asdict(p) for p in store.projects.values()], key=lambda x: x["last_meeting"], reverse=True)

@app.get("/api/summary/{meeting_id}")
async def get_summary(meeting_id: str):
    if meeting_id not in store.meetings:
        raise HTTPException(status_code=404, detail="Meeting not found")
    m = store.meetings[meeting_id]
    if not m.summary_content:
        return {"exists": False}
    return {"exists": True, "summary": m.summary_content}

def search_meetings(query: str, max_results: int = 10) -> List[tuple]:
    """íšŒì˜ë¡ ê²€ìƒ‰ - í‚¤ì›Œë“œ ë§¤ì¹­ + ê´€ë ¨ì„± ì ìˆ˜ (ê°œì„ ë¨ v3)"""
    query_lower = query.lower()
    
    # í•œêµ­ì–´ ì¡°ì‚¬ íŒ¨í„´ (ë‹¨ì–´ ëì—ì„œ ì œê±°)
    korean_particles = ['ì—ì„œ', 'ì—ê²Œ', 'ìœ¼ë¡œ', 'ì´ë‘', 'í•˜ê³ ', 'ë¼ê³ ', 'ë‹ˆê¹Œ', 'ì§€ë§Œ', 'ëŠ”ë°', 'ì—ì„œëŠ”', 'ì—ê²ŒëŠ”',
                        'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ëŠ”', 'ì€', 'ì—', 'ë¡œ', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€']
    
    def remove_particles(word):
        """ë‹¨ì–´ ëì˜ ì¡°ì‚¬ ì œê±°"""
        for particle in sorted(korean_particles, key=len, reverse=True):  # ê¸´ ì¡°ì‚¬ë¶€í„° ì²˜ë¦¬
            if word.endswith(particle) and len(word) > len(particle):
                return word[:-len(particle)]
        return word
    
    # í‚¤ì›Œë“œ ë¶„ë¦¬ ë° ì¡°ì‚¬ ì œê±°
    raw_keywords = [kw.strip() for kw in query_lower.split() if len(kw.strip()) >= 2]
    keywords = []
    for kw in raw_keywords:
        cleaned = remove_particles(kw)
        if len(cleaned) >= 2:
            keywords.append(cleaned)
        # ì›ë³¸ë„ ì¶”ê°€ (ì¡°ì‚¬ í¬í•¨ëœ í˜•íƒœë¡œ ê²€ìƒ‰ë  ìˆ˜ë„ ìˆìŒ)
        if kw not in keywords and len(kw) >= 2:
            keywords.append(kw)
    
    # ì¤‘ë³µ ì œê±°
    keywords = list(dict.fromkeys(keywords))
    
    # ì—°ì†ëœ ë‹¨ì–´ ì¡°í•©ë„ ê²€ìƒ‰ì–´ë¡œ ì¶”ê°€ (ì˜ˆ: "í•´ì € ì¼€ì´ë¸”" -> ["í•´ì €", "ì¼€ì´ë¸”", "í•´ì € ì¼€ì´ë¸”"])
    if len(keywords) >= 2:
        for i in range(len(keywords) - 1):
            combined = keywords[i] + " " + keywords[i + 1]
            if combined not in keywords:
                keywords.append(combined)
    
    # ë””ë²„ê¹… ë¡œê·¸
    print(f"[ê²€ìƒ‰] ì¿¼ë¦¬: '{query}' â†’ í‚¤ì›Œë“œ: {keywords}")
    
    scored_meetings = []
    
    for m in store.meetings.values():
        # ê²€ìƒ‰ ëŒ€ìƒ: ì œëª© + í´ë”ëª… + ì›ë³¸ + ìš”ì•½ë³¸
        title_lower = m.title.lower()
        folder_lower = m.folder.lower()
        content_lower = m.content.lower()
        summary_lower = m.summary_content.lower() if m.summary_content else ""
        
        search_text = f"{title_lower} {folder_lower} {content_lower} {summary_lower}"
        
        # ì ìˆ˜ ê³„ì‚°
        score = 0
        matched_keywords = []
        
        for kw in keywords:
            if kw in search_text:
                matched_keywords.append(kw)
                
                # ì œëª©ì— ìˆìœ¼ë©´ ìµœê³  ê°€ì¤‘ì¹˜
                if kw in title_lower:
                    score += 10
                # í´ë”ëª…ì— ìˆìœ¼ë©´ ë†’ì€ ê°€ì¤‘ì¹˜
                elif kw in folder_lower:
                    score += 8
                # ìš”ì•½ë³¸ì— ìˆìœ¼ë©´ ì¤‘ê°„ ê°€ì¤‘ì¹˜
                elif kw in summary_lower:
                    score += 5
                # ì›ë³¸ì— ìˆìœ¼ë©´ ê¸°ë³¸ ì ìˆ˜
                elif kw in content_lower:
                    score += 2
                
                # í‚¤ì›Œë“œê°€ ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•˜ë©´ ì¶”ê°€ ì ìˆ˜
                count = search_text.count(kw)
                if count > 1:
                    score += min(count - 1, 5)  # ìµœëŒ€ 5ì  ì¶”ê°€
        
        # í”„ë¡œì íŠ¸ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤
        if m.project and m.project.lower() in query_lower:
            score += 10
        
        # ëª¨ë“  í‚¤ì›Œë“œê°€ ë§¤ì¹­ë˜ë©´ í° ë³´ë„ˆìŠ¤ (ë” ì •í™•í•œ ê²°ê³¼)
        if len(keywords) > 1 and all(kw in search_text for kw in keywords):
            score += 20
        
        # ì •í™•í•œ êµ¬ë¬¸ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (ì˜ˆ: "í•´ì € ì¼€ì´ë¸”"ì´ ê·¸ëŒ€ë¡œ ìˆìœ¼ë©´)
        if query_lower in search_text:
            score += 15
        
        if score > 0:
            scored_meetings.append((m, score, matched_keywords))
    
    # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ Nê°œ
    scored_meetings.sort(key=lambda x: (-x[1], x[0].date), reverse=False)
    
    # ë””ë²„ê¹…: ìƒìœ„ ê²°ê³¼ ì¶œë ¥
    if scored_meetings:
        print(f"[ê²€ìƒ‰ ê²°ê³¼] ìƒìœ„ {min(5, len(scored_meetings))}ê°œ:")
        for m, score, kws in scored_meetings[:5]:
            print(f"  - {m.title} (ì ìˆ˜: {score}, ë§¤ì¹­: {kws})")
    else:
        print("[ê²€ìƒ‰ ê²°ê³¼] ë§¤ì¹­ëœ íšŒì˜ë¡ ì—†ìŒ")
    
    return scored_meetings[:max_results]

def search_worklog(query: str, worklog_data: dict, max_results: int = 5) -> List[dict]:
    """ì—…ë¬´ì¼ì§€ ê²€ìƒ‰ - í‚¤ì›Œë“œ ë§¤ì¹­"""
    if not worklog_data:
        return []
    
    query_lower = query.lower()
    keywords = [kw.strip() for kw in query_lower.split() if len(kw.strip()) >= 2]
    
    scored_worklogs = []
    
    for date_str, day_data in worklog_data.items():
        items = day_data.get('items', [])
        memo = day_data.get('memo', '')
        
        # ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ êµ¬ì„±
        items_text = ' '.join([item.get('content', '') for item in items]).lower()
        memo_lower = memo.lower() if memo else ''
        search_text = f"{items_text} {memo_lower}"
        
        if not search_text.strip():
            continue
        
        score = 0
        for kw in keywords:
            if kw in search_text:
                score += search_text.count(kw) * 2
        
        # ì •í™•í•œ êµ¬ë¬¸ ë§¤ì¹­ ë³´ë„ˆìŠ¤
        if query_lower in search_text:
            score += 10
        
        if score > 0:
            scored_worklogs.append({
                'date': date_str,
                'items': items,
                'memo': memo,
                'score': score
            })
    
    # ì ìˆ˜ìˆœ ì •ë ¬
    scored_worklogs.sort(key=lambda x: -x['score'])
    return scored_worklogs[:max_results]

@app.post("/api/chat")
async def chat(request: dict):
    query = request.get("query", "")
    worklog_data = request.get("worklog", {})  # ì—…ë¬´ì¼ì§€ ë°ì´í„° ë°›ê¸°
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        return {"answer": "ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "sources": []}
    
    # 1. í‚¤ì›Œë“œ ê²€ìƒ‰
    keyword_results = search_meetings(query, max_results=7)
    
    # 2. ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰
    semantic_results = semantic_search(query, n_results=7)
    print(f"[ê²€ìƒ‰] í‚¤ì›Œë“œ: {len(keyword_results)}ê°œ, ì‹œë§¨í‹±: {len(semantic_results)}ê°œ")
    
    # 3. ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°, ì ìˆ˜ í•©ì‚°)
    combined_scores = {}
    for meeting, score, kws in keyword_results:
        combined_scores[meeting.id] = {
            'meeting': meeting,
            'keyword_score': score,
            'semantic_score': 0,
            'matched': kws
        }
    
    for meeting, score, kws in semantic_results:
        if meeting.id in combined_scores:
            combined_scores[meeting.id]['semantic_score'] = score
        else:
            combined_scores[meeting.id] = {
                'meeting': meeting,
                'keyword_score': 0,
                'semantic_score': score,
                'matched': kws
            }
    
    # 4. ìµœì¢… ì ìˆ˜ ê³„ì‚° (í‚¤ì›Œë“œ 60% + ì‹œë§¨í‹± 40%)
    search_results = []
    for mid, data in combined_scores.items():
        final_score = data['keyword_score'] * 0.6 + data['semantic_score'] * 0.4
        search_results.append((data['meeting'], final_score, data['matched']))
    
    # ì ìˆ˜ìˆœ ì •ë ¬
    search_results.sort(key=lambda x: -x[1])
    search_results = search_results[:10]  # ìƒìœ„ 10ê°œ
    
    # ì—…ë¬´ì¼ì§€ ê²€ìƒ‰
    worklog_results = search_worklog(query, worklog_data, max_results=5)
    
    if not search_results and not worklog_results:
        return {"answer": "ê´€ë ¨ëœ íšŒì˜ë¡ì´ë‚˜ ì—…ë¬´ì¼ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.", "sources": []}
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_parts = []
    sources = []
    
    # íšŒì˜ë¡ ì»¨í…ìŠ¤íŠ¸
    for meeting, score, matched_kw in search_results:
        # ìš”ì•½ë³¸ì´ ìˆìœ¼ë©´ ìš”ì•½ë³¸ ìš°ì„ , ì—†ìœ¼ë©´ ì›ë³¸ ì¼ë¶€
        content_to_use = meeting.summary_content if meeting.summary_content else meeting.content[:2000]
        
        context_parts.append(f"""
[ë¬¸ì„œ ìœ í˜•: íšŒì˜ë¡]
[ë¬¸ì„œ ID: {meeting.id}]
[ì œëª©: {meeting.title}]
[ë‚ ì§œ: {meeting.date}]
[í´ë”: {meeting.folder}]
---
{content_to_use}
""")
        sources.append({
            "id": meeting.id, 
            "title": meeting.title, 
            "date": meeting.date,
            "folder": meeting.folder,
            "type": "meeting",
            "relevance": score
        })
    
    # ì—…ë¬´ì¼ì§€ ì»¨í…ìŠ¤íŠ¸
    for wl in worklog_results:
        items_text = '\n'.join([f"- [{item.get('status', 'pending')}] {item.get('content', '')}" for item in wl['items']])
        memo_text = wl['memo'] if wl['memo'] else '(ë©”ëª¨ ì—†ìŒ)'
        
        context_parts.append(f"""
[ë¬¸ì„œ ìœ í˜•: ì—…ë¬´ì¼ì§€]
[ë‚ ì§œ: {wl['date']}]
---
ğŸ“‹ ì—…ë¬´ í•­ëª©:
{items_text if items_text else '(ì—…ë¬´ í•­ëª© ì—†ìŒ)'}

ğŸ“ ë©”ëª¨:
{memo_text}
""")
        sources.append({
            "id": f"worklog_{wl['date']}", 
            "title": f"ì—…ë¬´ì¼ì§€ ({wl['date']})", 
            "date": wl['date'],
            "folder": "ì—…ë¬´ì¼ì§€",
            "type": "worklog",
            "relevance": wl['score']
        })

    # ì¡°ì§ ë°°ê²½ ì •ë³´ í¬í•¨
    org_info_section = f"""
## ğŸ“‹ ì¡°ì§ ë°°ê²½ ì •ë³´ (ì°¸ê³ ìš©)
{org_context}
""" if org_context else ""

    system_prompt = f"""ë‹¹ì‹ ì€ íšŒì˜ë¡ ë° ì—…ë¬´ì¼ì§€ ë¶„ì„ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ íšŒì˜ë¡ê³¼ ì—…ë¬´ì¼ì§€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
{org_info_section}
**âš ï¸ í•„ìˆ˜ ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**
1. ë°˜ë“œì‹œ ì œê³µëœ íšŒì˜ë¡/ì—…ë¬´ì¼ì§€ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. **[ì¶œì²˜ ëª…ì‹œ í•„ìˆ˜]** ëª¨ë“  ì •ë³´ì—ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”!
   - íšŒì˜ë¡: **(ì¶œì²˜: [íšŒì˜ ì œëª©], [ë‚ ì§œ])**
   - ì—…ë¬´ì¼ì§€: **(ì¶œì²˜: ì—…ë¬´ì¼ì§€, [ë‚ ì§œ])**
3. **[ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³  ìë£Œ ëª©ë¡ í•„ìˆ˜]** ë‹µë³€ ëì— ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì°¸ê³ í•œ ìë£Œë¥¼ ë‚˜ì—´í•˜ì„¸ìš”:
   ---
   ğŸ“Œ **ì°¸ê³  ìë£Œ:**
   - [íšŒì˜ ì œëª©] (ë‚ ì§œ, í´ë”)
   - ì—…ë¬´ì¼ì§€ (ë‚ ì§œ)
4. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ê°€ ìˆë‹¤ë©´ ê°ê°ì˜ ì¶œì²˜ë¥¼ ê°œë³„ì ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.
5. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ "ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”.
6. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
7. ì¡°ì§ ë°°ê²½ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ìš©ì–´, í”„ë¡œì íŠ¸ëª…, í”„ë¡œì„¸ìŠ¤ë¥¼ ì •í™•íˆ ì´í•´í•˜ê³  ë‹µë³€í•˜ì„¸ìš”."""

    user_prompt = f"""ì•„ë˜ëŠ” ê²€ìƒ‰ëœ íšŒì˜ë¡ê³¼ ì—…ë¬´ì¼ì§€ì…ë‹ˆë‹¤:

{"="*50}
{chr(10).join(context_parts)}
{"="*50}

ì§ˆë¬¸: {query}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
âš ï¸ ì¤‘ìš”: ëª¨ë“  ì •ë³´ì— ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ê³ , ë‹µë³€ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ "ğŸ“Œ ì°¸ê³  ìë£Œ:" ëª©ë¡ì„ í¬í•¨í•˜ì„¸ìš”!"""

    async with httpx.AsyncClient(timeout=90.0) as client:
        try:
            response = await client.post(
                ANTHROPIC_API_URL,
                headers={
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json"
                },
                json={
                    "model": CLAUDE_MODEL,
                    "max_tokens": 2048,
                    "system": system_prompt,
                    "messages": [{"role": "user", "content": user_prompt}]
                }
            )
            result = response.json()
            
            # ë””ë²„ê¹…ìš© ë¡œê·¸
            print(f"API Response Status: {response.status_code}")
            
            # ì—ëŸ¬ ì²´í¬
            if response.status_code != 200:
                error_msg = result.get('error', {})
                if isinstance(error_msg, dict):
                    error_msg = error_msg.get('message', str(result))
                return {"answer": f"API ì˜¤ë¥˜ ({response.status_code}): {error_msg}", "sources": sources}
            
            if "content" in result and len(result["content"]) > 0:
                answer = result["content"][0]["text"]
            else:
                # ì „ì²´ ì‘ë‹µ êµ¬ì¡° í™•ì¸
                print(f"Unexpected response: {result}")
                answer = "ì‘ë‹µì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
            return {"answer": answer, "sources": sources}
        except Exception as e:
            print(f"Chat API Error: {str(e)}")
            return {"answer": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "sources": []}

def find_available_port(preferred_port=8000, max_attempts=10):
    """
    âš ï¸ í¬íŠ¸ ì„¤ì • ê·œì¹™ (ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€):
    - Milestone Tracker: 8000
    - Money Advisor: 8020  
    - Money Manage: 8030
    """
    """ì‚¬ìš© ê°€ëŠ¥í•œ í¬íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
    import socket
    
    for offset in range(max_attempts):
        port = preferred_port + offset
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('127.0.0.1', port))
                return port
        except OSError:
            print(f"[í¬íŠ¸ {port}] ì´ë¯¸ ì‚¬ìš© ì¤‘, ë‹¤ìŒ í¬íŠ¸ ì‹œë„...")
            continue
    
    raise RuntimeError(f"ì‚¬ìš© ê°€ëŠ¥í•œ í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({preferred_port}-{preferred_port + max_attempts - 1})")


def open_browser_delayed(port, delay=3):
    """ì„œë²„ ì‹œì‘ í›„ ë¸Œë¼ìš°ì €ì—ì„œ localhostë¡œ ì—½ë‹ˆë‹¤."""
    import threading
    import webbrowser
    import time
    
    def open_browser():
        time.sleep(delay)
        webbrowser.open(f"http://localhost:{port}")
    
    thread = threading.Thread(target=open_browser)
    thread.daemon = True
    thread.start()


if __name__ == "__main__":
    port = find_available_port(preferred_port=8000)
    html_path = PRACTICE_DIR / "milestone_tracker.html"
    
    print(f"\n{'='*50}")
    print(f"  Milestone Tracker ì„œë²„ ì‹œì‘: http://localhost:{port}")
    print(f"  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ ì•½ 30ì´ˆ, ì´í›„ ë¹ ë¦„)")
    print(f"  ì´ ì°½ì„ ë‹«ìœ¼ë©´ ì„œë²„ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print(f"{'='*50}\n")
    
    open_browser_delayed(port)
    uvicorn.run(app, host="0.0.0.0", port=port)
